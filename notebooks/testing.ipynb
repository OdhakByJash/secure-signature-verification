{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2903a466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9f25f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Active-Repositories\\secure-signature-verification\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Active-Repositories\\secure-signature-verification\\venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.98%\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),  \n",
    "    transforms.ToTensor(),  \n",
    "])\n",
    "class SiameseSignatureDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        self.pairs = pairs\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        img1, img2, label = self.pairs[idx]\n",
    "        if not torch.is_tensor(img1):\n",
    "            img1 = transform(img1)\n",
    "        if not torch.is_tensor(img2):\n",
    "            img2 = transform(img2)\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "        return img1, img2, label\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        resnet = models.resnet18(pretrained=False)\n",
    "        resnet.fc = nn.Identity()\n",
    "        self.base_model = resnet  \n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128)\n",
    "        )\n",
    "    def forward_once(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "    def forward(self, x1, x2):\n",
    "        return self.forward_once(x1), self.forward_once(x2)\n",
    "with open(\"C:/Active-Repositories/secure-signature-verification/image_pairs/positive_pairs.pkl\", \"rb\") as f1:\n",
    "    positive_pairs = pickle.load(f1)\n",
    "with open(\"C:/Active-Repositories/secure-signature-verification/image_pairs/negative_pairs.pkl\", \"rb\") as f2:\n",
    "    negative_pairs = pickle.load(f2)\n",
    "all_pairs = positive_pairs + negative_pairs\n",
    "random.shuffle(all_pairs)\n",
    "split = int(0.8 * len(all_pairs))\n",
    "train_pairs = all_pairs[:split]\n",
    "test_pairs = all_pairs[split:]\n",
    "test_dataset = SiameseSignatureDataset(test_pairs)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SiameseNetwork().to(device)\n",
    "parameters_file_path = \"C:/Active-Repositories/secure-signature-verification/trained_model_parameters/siamese_resnet_18.pth\"\n",
    "model.load_state_dict(torch.load(parameters_file_path, map_location=device))\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "threshold = 0.4275 \n",
    "with torch.no_grad():\n",
    "    for img1, img2, labels in test_loader:\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        out1, out2 = model(img1, img2)\n",
    "        distances = F.pairwise_distance(out1, out2)\n",
    "        predictions = (distances < threshold).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "test_accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
